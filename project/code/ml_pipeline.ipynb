{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os as os\n",
    "from time import time, strftime\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, explained_variance_score, r2_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classes for Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Load_Data(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "        self.weather_dir = ''\n",
    "        self.soil_dir = ''\n",
    "        self.drop_columns = ['STATION', 'NAME', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'AWND_ATTRIBUTES', 'PGTM_ATTRIBUTES', \n",
    "                             'PSUN', 'PSUN_ATTRIBUTES', 'SNOW', 'SNOW_ATTRIBUTES', 'SNWD', 'SNWD_ATTRIBUTES', 'TAVG',\n",
    "                             'TAVG_ATTRIBUTES', 'TMAX_ATTRIBUTES', 'TMIN_ATTRIBUTES', 'TSUN', 'TSUN_ATTRIBUTES', 'WDF2_ATTRIBUTES', \n",
    "                             'WDF5_ATTRIBUTES', 'WSF2_ATTRIBUTES','WSF5_ATTRIBUTES', 'WT01_ATTRIBUTES', 'WT02_ATTRIBUTES', \n",
    "                             'WT03_ATTRIBUTES', 'WT06_ATTRIBUTES', 'WT08_ATTRIBUTES', 'PRCP_ATTRIBUTES']\n",
    "        \n",
    "    def fit(self, w_dir, s_dir):\n",
    "        self.weather_dir = w_dir\n",
    "        self.soil_dir = s_dir\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #Aggregate all 43 files into one file\n",
    "        file_list = os.listdir(self.soil_dir)\n",
    "        agg_data = pd.DataFrame()\n",
    "        for file in file_list:\n",
    "            path = self.soil_dir + file\n",
    "            curr_data = pd.read_csv(path, sep='\\t')\n",
    "            agg_data = agg_data.append(curr_data)\n",
    "        \n",
    "        #Drop rows with only NAs for measurement values\n",
    "        soil = agg_data.dropna(thresh=10)\n",
    "        \n",
    "        #Import weather files and drop unnessecary fields\n",
    "        weather = pd.read_csv(self.weather_dir)\n",
    "        drop_cols = list(set(weather.columns).intersection(self.drop_columns))\n",
    "        weather = weather.drop(columns = self.drop_columns)\n",
    "        \n",
    "        #Convert both files to use same datetime\n",
    "        soil['Date'] = pd.to_datetime(soil['Date'])\n",
    "        weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "        \n",
    "        #Join previous 16 days weather to moisture readings\n",
    "        for i in range(0, 17):\n",
    "            weather_new = weather.add_suffix('_' + str(i))\n",
    "            soil = soil.merge(weather_new, how = 'left', left_on = 'Date', right_on = weather['DATE'] - pd.DateOffset(i * -1))\n",
    "            \n",
    "        #Store the month of the reading as a feature\n",
    "        soil['Month'] = pd.DatetimeIndex(soil['Date']).month\n",
    "        \n",
    "        date_attribs = ['Date', 'DATE_0', 'DATE_1', 'DATE_2', 'DATE_3', 'DATE_4','DATE_5', 'DATE_6', 'DATE_7', 'DATE_8', 'DATE_9', 'DATE_10', \\\n",
    "                        'DATE_11', 'DATE_12', 'DATE_13', 'DATE_14', 'DATE_15', 'DATE_16']\n",
    "        \n",
    "        if 'DATE_0' in list(soil.columns):\n",
    "            soil.drop(columns = date_attribs, inplace = True)\n",
    "        soil['Location'] = soil['Location'].astype('object')\n",
    "            \n",
    "        return soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Engineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #Add categorical feature that simply stores if it rained that day or not\n",
    "        for i in range(17):\n",
    "            col_name = 'PRCP_' + str(i)\n",
    "            rain_y_n_name = 'RAIN_Y_N_' + str(i)\n",
    "            X[rain_y_n_name] = np.nan\n",
    "            X[rain_y_n_name].loc[X[col_name] > 0] = 1\n",
    "            X[rain_y_n_name].loc[X[col_name] == 0] = 0\n",
    "            X[rain_y_n_name] = X[rain_y_n_name].astype('object')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print(X)\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert_Date(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names = None):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X['Date'] = pd.to_timedelta(X['Date']).dt.total_seconds().astype(int)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pipeline That Uses Data Processing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "soil_file_dir = '../data/soil/'\n",
    "weather_file_dir = '../data/weather/weather_data.csv'\n",
    "x = 0\n",
    "\n",
    "pre_work_pipeline = Pipeline([\n",
    "    ('prework', Load_Data()),\n",
    "    ('features', Feature_Engineer())\n",
    "])\n",
    "\n",
    "pre_work_pipeline.fit(weather_file_dir, soil_file_dir)\n",
    "prework_df = pre_work_pipeline.transform(x)\n",
    "#Save to CSV so that we do not need to import and clean data everytime\n",
    "prework_df.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Classifier Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols = ['VW_30cm', 'VW_60cm', 'VW_90cm', 'VW_120cm', 'VW_150cm']\n",
    "\n",
    "for cols in y_cols:\n",
    "    name = cols[3:] + '_class'\n",
    "    prework_df[name] = ''\n",
    "    prework_df[name].loc[(prework_df[cols] <= 0.1)] = '0.1'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.1) & (prework_df[cols] <= 0.2)] = '0.2'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.2) & (prework_df[cols] <= 0.3)] = '0.3'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.3) & (prework_df[cols] <= 0.4)] = '0.4'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.4) & (prework_df[cols] <= 0.5)] = '0.5'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.5) & (prework_df[cols] <= 0.6)] = '0.6'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.6) & (prework_df[cols] <= 0.7)] = '0.7'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.7) & (prework_df[cols] <= 0.8)] = '0.8'\n",
    "    prework_df[name].loc[(prework_df[cols] > 0.8)] = '0.9'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data Frames for Each Depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moisture data is taken at various depths. We want to build models seperately for different depths. So we need to make a dataframe for each depth so that we can elminate entire rows where the predictor is NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split out y values\n",
    "all_y_cols = ['VW_30cm', 'VW_60cm', 'VW_90cm', 'VW_120cm', 'VW_150cm', '30cm_class', '60cm_class', '90cm_class', '120cm_class', '150cm_class']\n",
    "X_sets = {}\n",
    "y_sets = {}\n",
    "x_cols = [col for col in prework_df.columns if col not in y_cols]\n",
    "X = prework_df.loc[:, x_cols]\n",
    "#y = prework_df.loc[:, y_cols]\n",
    "\n",
    "for cols in all_y_cols:\n",
    "    if cols[:1] == 'V':\n",
    "        dataset_name = cols[3:]\n",
    "    else:\n",
    "        dataset_name = cols\n",
    "    holder = prework_df.dropna(subset = [cols])\n",
    "    X_sets[dataset_name] = holder[x_cols].fillna(0)\n",
    "    y_sets[dataset_name] = holder[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training and test data\n",
    "# 80-20 ratio\n",
    "# Trying to keep same ratios for each location using stratify\n",
    "# Could have done this in the cell above, but wanted a seperate step for this\n",
    "X_train_set = {}\n",
    "X_test_set = {}\n",
    "y_train_set = {}\n",
    "y_test_set = {}\n",
    "\n",
    "for cols in all_y_cols:\n",
    "    if cols[:1] == 'V':\n",
    "        dataset_name = cols[3:]\n",
    "    else:\n",
    "        dataset_name = cols \n",
    "    X_train_set[dataset_name], X_test_set[dataset_name], y_train_set[dataset_name], y_test_set[dataset_name] = train_test_split(X_sets[dataset_name], y_sets[dataset_name], \\\n",
    "                                                                                                                                test_size=0.2, stratify = X_sets[dataset_name]['Location'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = X_train_set['60cm'].select_dtypes(exclude=['object', 'category']).columns\n",
    "cat_attribs = X_train_set['60cm'].select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = 0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value = '')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_attribs),\n",
    "        ('cat', categorical_transformer, cat_attribs)\n",
    "    ])\n",
    "\n",
    "pipe_with_estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', RandomForestRegressor())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=0,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='constant',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler(copy=True,\n",
       "                                                                                  with_mean=...\n",
       "                                   verbose=False)),\n",
       "                ('classifier',\n",
       "                 RandomForestRegressor(bootstrap=True, criterion='mse',\n",
       "                                       max_depth=None, max_features='auto',\n",
       "                                       max_leaf_nodes=None,\n",
       "                                       min_impurity_decrease=0.0,\n",
       "                                       min_impurity_split=None,\n",
       "                                       min_samples_leaf=1, min_samples_split=2,\n",
       "                                       min_weight_fraction_leaf=0.0,\n",
       "                                       n_estimators=10, n_jobs=None,\n",
       "                                       oob_score=False, random_state=None,\n",
       "                                       verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_with_estimator.fit(X_train_set['60cm'], y_train_set['60cm'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9013707703385528"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_score(y_test_set['60cm'], pipe_with_estimator.predict(X_test_set['60cm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Experiment  Depth   Fit_Time  Pred_Time  r2_score  exp_var_score  \\\n",
      "0  First Linear Reg   30cm  58.701572   0.238179  0.890208       0.890208   \n",
      "1  First Linear Reg   60cm  58.847758   0.169551  0.898520       0.898522   \n",
      "2  First Linear Reg   90cm  54.927837   0.172255  0.882179       0.882180   \n",
      "3  First Linear Reg  120cm  64.877650   0.197685  0.884033       0.884033   \n",
      "4  First Linear Reg  150cm  63.256277   0.170247  0.876900       0.876920   \n",
      "\n",
      "              datetime  \n",
      "0  2020-11-02 20:10:26  \n",
      "1  2020-11-02 20:11:25  \n",
      "2  2020-11-02 20:12:20  \n",
      "3  2020-11-02 20:13:25  \n",
      "4  2020-11-02 20:14:28  \n"
     ]
    }
   ],
   "source": [
    "data_cols = ['30cm', '60cm', '90cm', '120cm', '150cm']\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    log = pd.DataFrame(columns = ['Experiment', 'Depth', 'Fit_Time', 'Pred_Time', 'r2_score', 'exp_var_score', 'datetime'])\n",
    "    \n",
    "for cols in data_cols:\n",
    "    t0 = time()\n",
    "    pipe_with_estimator.fit(X_train_set[cols], y_train_set[cols])\n",
    "    t1 = time()\n",
    "    preds = pipe_with_estimator.predict(X_test_set[cols])\n",
    "    t2 = time()\n",
    "    expvar = explained_variance_score(y_test_set[cols], preds)\n",
    "    r2sc = r2_score(y_test_set[cols], preds)\n",
    "    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log.loc[len(log)] = ['First Linear Reg', cols, t1-t0, t2-t1, r2sc, expvar, now]\n",
    "    \n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Different Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from notebook from INFO526 SP2020\n",
    "def ConductGridSearch(X_train, y_train, X_test, y_test, i=0, prefix='', n_jobs=-1, verbose=1):    \n",
    "    classifiers = [\n",
    "        ('Logistic Regression', LogisticRegression(random_state=42, max_iter=4000, multi_class = 'multinomial', solver = 'newton-cg')),\n",
    "        #('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "        #('Support Vector', SVC(random_state=42)),\n",
    "        #('Stochastic GD', SGDClassifier(random_state=42)),\n",
    "        #('RandomForest', RandomForestClassifier()),\n",
    "    ]\n",
    "\n",
    "    # Arrange grid search parameters for each classifier\n",
    "    params_grid = {\n",
    "        'Logistic Regression': {\n",
    "            'penalty': ('l2', 'none'),\n",
    "            'tol': (0.0001, 0.00001, 0.0000001), \n",
    "            'C': (10, 1, 0.1, 0.01),\n",
    "        },\n",
    "        'K-Nearest Neighbors': {\n",
    "            'n_neighbors': (3, 5, 7, 8, 11),\n",
    "            'p': (1,2),\n",
    "        },\n",
    "        #'Naive Bayes': {},\n",
    "        'Support Vector' : {\n",
    "            'kernel': ('rbf', 'poly'),     \n",
    "            'degree': (1, 2, 3, 4, 5),\n",
    "            'C': (10, 1, 0.1, 0.01),\n",
    "        },\n",
    "        'Stochastic GD': {\n",
    "            'loss': ('hinge', 'perceptron', 'log'),\n",
    "            'penalty': ('l1', 'l2', 'elasticnet'),\n",
    "            'tol': (0.0001, 0.00001, 0.0000001), \n",
    "            'alpha': (0.1, 0.01, 0.001, 0.0001), \n",
    "        },\n",
    "        'RandomForest':  {\n",
    "            'max_depth': [9, 15, 22, 26, 30],\n",
    "            'max_features': [1, 3, 5],\n",
    "            'min_samples_split': [5, 10, 15],\n",
    "            'min_samples_leaf': [3, 5, 10],\n",
    "            'bootstrap': [False],\n",
    "            'n_estimators':[20, 80, 150, 200, 300]},\n",
    "    }\n",
    "    \n",
    "    for (name, classifier) in classifiers:\n",
    "        i += 1\n",
    "        # Print classifier and parameters\n",
    "        print('****** START',prefix, name,'*****')\n",
    "        parameters = params_grid[name]\n",
    "        print(\"Parameters:\")\n",
    "        for p in sorted(parameters.keys()):\n",
    "            print(\"\\t\"+str(p)+\": \"+ str(parameters[p]))\n",
    "        \n",
    "        # generate the pipeline\n",
    "        full_pipeline_with_predictor = Pipeline([\n",
    "        (\"preparation\", preprocessor),\n",
    "        (\"predictor\", classifier)\n",
    "        ])\n",
    "        \n",
    "        # Execute the grid search\n",
    "        params = {}\n",
    "        for p in parameters.keys():\n",
    "            pipe_key = 'predictor__'+str(p)\n",
    "            params[pipe_key] = parameters[p] \n",
    "        grid_search = GridSearchCV(full_pipeline_with_predictor, params, scoring='roc_auc', cv=5, \n",
    "                                   n_jobs=n_jobs, verbose=verbose)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "                \n",
    "        # Best estimator score\n",
    "        best_train = pct(grid_search.best_score_)\n",
    "\n",
    "        # Best estimator fitting time\n",
    "        start = time()\n",
    "        grid_search.best_estimator_.fit(x_train, y_train)\n",
    "        train_time = round(time() - start, 4)\n",
    "\n",
    "        # Best estimator prediction time\n",
    "        start = time()\n",
    "        best_test_accuracy = pct(grid_search.best_estimator_.score(X_test, y_test))\n",
    "        test_time = round(time() - start, 4)\n",
    "\n",
    "        best_train_scores = cross_val_score(grid_search.best_estimator_, x_train, y_train, cv=cv30Splits)\n",
    "        best_train_accuracy = pct(best_train_scores.mean())\n",
    " \n",
    "        (t_stat, p_value) = stats.ttest_rel(logit_scores, best_train_scores)\n",
    "        \n",
    "        # Collect the best parameters found by the grid search\n",
    "        print(\"Best Parameters:\")\n",
    "        best_parameters = grid_search.best_estimator_.get_params()\n",
    "        param_dump = []\n",
    "        for param_name in sorted(params.keys()):\n",
    "            param_dump.append((param_name, best_parameters[param_name]))\n",
    "            print(\"\\t\"+str(param_name)+\": \" + str(best_parameters[param_name]))\n",
    "        print(\"****** FINISH\",prefix,name,\" *****\")\n",
    "        print(\"\")\n",
    "        \n",
    "        # Record the results\n",
    "        results.loc[i] = [prefix+name, best_train_accuracy, best_test_accuracy, round(p_value,5), train_time, test_time, json.dumps(param_dump)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still trying to get this to work\n",
    "curr = '30cm_class'\n",
    "ConductGridSearch(X_train_set[curr], y_train_set[curr], X_test_set[curr], y_test_set[curr], 0, \"Best Model:\",  n_jobs=-1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
